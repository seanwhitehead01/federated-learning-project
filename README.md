
# Federated Learning with Model Editing: Mitigating Client Heterogeneity through Sparsity and Weighted Aggregation

This project presents the development and evaluation of models within a Federated Learning framework using the CIFAR-100 dataset. A centralized model is first implemented, followed by a federated version assessed under both IID and non-IID data distributions. Model editing with varying weight sparsity is applied in both settings to evaluate performance impacts. Additionally, the study examines client heterogeneity by simulating imbalanced data distributions across clients, with variations in class presence and dataset size. This scenario highlights the negative effects of â€˜harmfulâ€™ clients on the global model. To address these challenges, two corrective actions are applied: sparse fine-tuning to limit disruptive client updates, and weighted aggregation based on client dataset properties.

---

## ğŸ“ Repository Structure

```
federated-learning-project/
â”œâ”€â”€ checkpoints/                          # Directory containing the saved models
â”œâ”€â”€ data/                                 # Script to load the CIFAR-100 dataset
â”œâ”€â”€ dataset/                              # Directory containing the dataset
â”œâ”€â”€ model/                                # Scripts for model operations:
â”‚   â”œâ”€â”€ federated_averaging.py            # Federated averaging logic
â”‚   â”œâ”€â”€ model_editing.py                  # Model editing and sparsity management
â”‚   â”œâ”€â”€ hyperparameter_tuning.py          # Hyperparameter tuning routines
â”‚   â”œâ”€â”€ prepare_model.py                  # Model loading and layer freezing
â”‚   â””â”€â”€ unbalance.py                      # Tools for unbalanced data: weights, discrepancy, severity
â”œâ”€â”€ results/                              # Directory containing the results of the experiments
â”œâ”€â”€ scripts/                              # Jupyter notebooks to reproduce experiments
â”‚   â”œâ”€â”€ centralized_baseline.ipynb
â”‚   â”œâ”€â”€ centralized_model_editing.ipynb
â”‚   â”œâ”€â”€ federated_averaging.ipynb
â”‚   â”œâ”€â”€ federated_model_editing.ipynb
â”‚   â”œâ”€â”€ federated_unbalance_head.ipynb    # Contains personal contribution experiments
â”‚   â””â”€â”€ federated_unbalance_masked.ipynb  # Contains personal contribution experiments
â”œâ”€â”€ train.py                              # Standalone script to train a model
â”œâ”€â”€ eval.py                               # Standalone script to evaluate a model
â”œâ”€â”€ requirements.txt                      # Python dependencies for reproducibility
â””â”€â”€ README.md                             # This documentation
```

---

## ğŸš€ How to Reproduce the Experiments on Google Colab

All experiments can be executed in Google Colab. Some notebooks allow configuration of experiment parameters in the first cell (e.g., number of classes per client, sparsity levels, etc.).

### Step-by-step Instructions:

1. **Open [Google Colab](https://colab.research.google.com/)**
2. **Clone the repository** inside a new notebook or terminal cell:
   ```bash
   !git clone https://github.com/AlessandroMaini/federated-learning-project.git
   %cd federated-learning-project
   ```
3. **Install dependencies**:
   ```bash
   !pip install -r requirements.txt
   ```
4. **Run one of the notebooks** depending on the desired experiment:
   - `centralized_baseline.ipynb`: Centralized training baseline
   - `centralized_model_editing.ipynb`: Centralized model editing baseline
   - `federated_averaging.ipynb`: Federated learning with IID and non-IID clients
   - `federated_model_editing.ipynb`: Federated learning with model editing (both IID and non-IID cases)
   - `federated_unbalance_head.ipynb`: Federated learning in unbalanced scenario training only the head. Includes 2 modes:
     - Baseline
     - Discrepancy-aware weighted aggregation
   - `federated_unbalance_masked.ipynb`: Federated learning in unbalanced scenario with sparse fine-tuning. Includes multiple modes:
     - Baseline
     - Discrepancy-aware weighted aggregation
     - Severity-aware sparse fine-tuning

ğŸ”§ Customize parameters in the **first cell** of the notebooks to adjust:
- Number of classes per client
- Client masks density levels
- Experiment type (baseline, discrepancy-aware, severity-aware)

---

## ğŸ“š Notes

- All experiments are designed to run on **Google Colab**, with GPU acceleration enabled.
- CIFAR-100 is automatically loaded using provided dataset scripts.
- All required packages are listed in `requirements.txt`. Install them with:
  ```bash
  pip install -r requirements.txt
  ```

---
