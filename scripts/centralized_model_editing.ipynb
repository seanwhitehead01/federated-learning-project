{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Centralized scenario with Masked Aggregation.\n",
    "Change this cell to run the scenario with different parameters.\"\"\"\n",
    "\n",
    "# Density of the mask (percent of non-zero elements)\n",
    "density = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from data.cifar100_loader import get_cifar100_loaders\n",
    "from model.prepare_model import get_dino_vits16_model, freeze_backbone, unfreeze_backbone, freeze_head, unfreeze_head\n",
    "from model.hyperparameter_tuning import run_grid_search\n",
    "from eval import evaluate\n",
    "from train import train\n",
    "from model.model_editing import mask_calculator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06da0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Directories for data and checkpoints\n",
    "CHECKPOINT_DIR = './checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0860525",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_dino_vits16_model(device)\n",
    "# Freeze backbone\n",
    "freeze_backbone(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full training set (train + val)\n",
    "full_train_loader, _, test_loader = get_cifar100_loaders(val_split=0.0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ecbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform brief pre-training on the full training set\n",
    "start_epoch = 0\n",
    "warmup_epochs = 3\n",
    "best_test_acc = 0.0\n",
    "\n",
    "warmup_train_loss = []\n",
    "warmup_train_acc = []\n",
    "warmup_test_loss = []\n",
    "warmup_test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set best configuration found during grid search\n",
    "best_cfg = {\"lr\": 0.005, \"momentum\": 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8633df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=best_cfg['lr'], momentum=best_cfg['momentum'], weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=warmup_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c9c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run warmup training\n",
    "for epoch in range(start_epoch, start_epoch + warmup_epochs):\n",
    "    train_loss, train_acc = train(model, full_train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    warmup_train_loss.append(train_loss)\n",
    "    warmup_train_acc.append(train_acc)\n",
    "    warmup_test_loss.append(test_loss)\n",
    "    warmup_test_acc.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{start_epoch + warmup_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'pre_trained_model_centralized.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a126472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test loss\n",
    "plt.plot(warmup_train_loss, label='Train Loss')\n",
    "plt.plot(warmup_test_loss, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test accuracy\n",
    "plt.plot(warmup_train_acc, label='Train Accuracy')\n",
    "plt.plot(warmup_test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aab996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'pre_trained_model_centralized.pth')))\n",
    "# Unfreeze the backbone\n",
    "unfreeze_backbone(model)\n",
    "# Freeze the head\n",
    "freeze_head(model)\n",
    "\n",
    "# Define the number of samples per class for CIFAR-100\n",
    "samples_per_class = [5] * 100\n",
    "\n",
    "# Compute the mask\n",
    "mask = mask_calculator(model, full_train_loader.dataset, device, samples_per_class=samples_per_class, density=density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e51fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the mask\n",
    "start_epoch = 0\n",
    "num_epochs = 10\n",
    "best_test_acc = 0.0\n",
    "\n",
    "hist_train_loss = []\n",
    "hist_train_acc = []\n",
    "hist_test_loss = []\n",
    "hist_test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-set optimizer and scheduler for fine-tuning\n",
    "optimizer = optim.SGD(model.parameters(), lr=best_cfg['lr'], momentum=best_cfg['momentum'], weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the mask\n",
    "for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    train_loss, train_acc = train(model, full_train_loader, optimizer, criterion, device, grad_mask=mask)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    hist_train_loss.append(train_loss)\n",
    "    hist_train_acc.append(train_acc)\n",
    "    hist_test_loss.append(test_loss)\n",
    "    hist_test_acc.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{start_epoch + num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.4f}\")\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, os.path.join(CHECKPOINT_DIR, 'best_model_overall.pth'))\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, f'centralized_edited_epoch{epoch+1}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test loss\n",
    "plt.plot(hist_train_loss, label='Train Loss')\n",
    "plt.plot(hist_test_loss, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17133150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and test accuracy\n",
    "plt.plot(hist_train_acc, label='Train Accuracy')\n",
    "plt.plot(hist_test_acc, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
